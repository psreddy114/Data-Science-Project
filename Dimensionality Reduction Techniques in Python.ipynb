{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd84b9f8",
   "metadata": {},
   "source": [
    "Link: https://www.analyticsvidhya.com/blog/2018/08/dimensionality-reduction-techniques-python/?utm_source=courses&utm_medium=get_started_with_sklearn\n",
    "\n",
    "# Guide to 12 Dimensionality Reduction Techniques with Python Codes\n",
    "\n",
    "By using dimensionality reduction techniques we can reduce the number of features in dataset without losing much information and keep improving the model's performance. It's very powerful way to deal with huge datasets.\n",
    "\n",
    "Using dimensionality reduction techniques, of course. You can use this concept to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance. It’s a really powerful way to deal with huge datasets, as you’ll see in this article.\n",
    "\n",
    "This is a comprehensive guide to various dimensionality reduction techniques that can be used in practical scenarios. We will first understand what this concept is and why we should use it, before diving into the 12 different techniques I have covered. Each technique has it’s own implementation in Python to get you well acquainted with it.\n",
    "\n",
    "### Table of Contents\n",
    "1. What is Dimensionality Reduction?  \n",
    "2. Why is Dimensionality Reduction required?  \n",
    "3. Common Dimensionality Reduction Techniques  \n",
    "    3.1 Missing Value Ratio  \n",
    "    3.2 Low Variance Filter  \n",
    "    3.3 High Correlation Filter  \n",
    "    3.4 Random Forest  \n",
    "    3.5 Backward Feature Elimination  \n",
    "    3.6 Forward Feature Selection  \n",
    "    3.7 Factor Analysis  \n",
    "    3.8 Principal Component Analysis  \n",
    "    3.9 Independent Component Analysis  \n",
    "    3.10 Methods Based on Projections  \n",
    "    3.11 t-Distributed Stochastic Neighbor Embedding (t-SNE)  \n",
    "    3.12 UMAP  \n",
    "4. Applications of Various Dimensionality Reduction Techniques\n",
    "\n",
    "## 1. What is Dimensionality Reduction?\n",
    "\n",
    "We are generating a tremendous amount of data daily. In fact, 90% of the data in the world has been generated in the last 3-4 years! The numbers are truly mind boggling. Below are just some of the examples of the kind of data being collected:\n",
    "\n",
    "Facebook collects data of what you like, share, post, places you visit, restaurants you like, etc.\n",
    "Your smartphone apps collect a lot of personal information about you\n",
    "Amazon collects data of what you buy, view, click, etc. on their site\n",
    "Casinos keep a track of every move each customer makes\n",
    "As data generation and collection keeps increasing, visualizing it and drawing inferences becomes more and more challenging. One of the most common ways of doing visualization is through charts. Suppose we have 2 variables, Age and Height. We can use a scatter or line plot between Age and Height and visualize their relationship easily:\n",
    "\n",
    "Now consider a case in which we have, say 100 variables (p=100). In this case, we can have 100(100-1)/2 = 5000 different plots. It does not make much sense to visualize each of them separately, right? In such cases where we have a large number of variables, it is better to select a subset of these variables (p<<100) which captures as much information as the original set of variables.\n",
    "\n",
    "Here we have weights of similar objects in Kg (X1) and Pound (X2). If we use both of these variables, they will convey similar information. So, it would make sense to use only one variable. We can convert the data from 2D (X1 and X2) to 1D (Y1) as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eed1d5",
   "metadata": {},
   "source": [
    "# 2. Why is Dimensionality Reduction required?\n",
    "Here are some of the benefits of applying dimensionality reduction to a dataset:\n",
    "\n",
    "Space required to store the data is reduced as the number of dimensions comes down\n",
    "Less dimensions lead to less computation/training time\n",
    "Some algorithms do not perform well when we have a large dimensions. So reducing these dimensions needs to happen for the algorithm to be useful\n",
    "It takes care of multicollinearity by removing redundant features. For example, you have two variables – ‘time spent on treadmill in minutes’ and ‘calories burnt’. These variables are highly correlated as the more time you spend running on a treadmill, the more calories you will burn. Hence, there is no point in storing both as just one of them does what you require\n",
    "It helps in visualizing data. As discussed earlier, it is very difficult to visualize data in higher dimensions so reducing our space to 2D or 3D may allow us to plot and observe patterns more clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131300ee",
   "metadata": {},
   "source": [
    "# 3. Common Dimensionality Reduction Techniques\n",
    "Dimensionality reduction can be done in two different ways:\n",
    "\n",
    "By only keeping the most relevant variables from the original dataset (this technique is called feature selection)\n",
    "By finding a smaller set of new variables, each being a combination of the input variables, containing basically the same information as the input variables (this technique is called dimensionality reduction)\n",
    "We will now look at various dimensionality reduction techniques and how to implement each of them in Python.\n",
    "\n",
    " \n",
    "\n",
    "## 3.1 Missing Value Ratio\n",
    "Suppose you’re given a dataset. What would be your first step? You would naturally want to explore the data first before building model. While exploring the data, you find that your dataset has some missing values. Now what? You will try to find out the reason for these missing values and then impute them or drop the variables entirely which have missing values (using appropriate methods).\n",
    "\n",
    "What if we have too many missing values (say more than 50%)? Should we impute the missing values or drop the variable? I would prefer to drop the variable since it will not have much information. However, this isn’t set in stone. We can set a threshold value and if the percentage of missing values in any variable is more than that threshold, we will drop the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580a7b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required Libraries in Python\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47600c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset in to Python\n",
    "\n",
    "train = pd.read_csv(\"C:\\\\Users\\\\SHASHI\\\\OneDrive\\\\Desktop\\\\Python\\\\Dimensionality Reduction Techniques\\\\Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec0f0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.30</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.1380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRC01</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.019278</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>443.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDN15</td>\n",
       "      <td>17.50</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016760</td>\n",
       "      <td>Meat</td>\n",
       "      <td>141.6180</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2097.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDX07</td>\n",
       "      <td>19.20</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Fruits and Vegetables</td>\n",
       "      <td>182.0950</td>\n",
       "      <td>OUT010</td>\n",
       "      <td>1998</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Grocery Store</td>\n",
       "      <td>732.3800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCD19</td>\n",
       "      <td>8.93</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Household</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>994.7052</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility  \\\n",
       "0           FDA15         9.30          Low Fat         0.016047   \n",
       "1           DRC01         5.92          Regular         0.019278   \n",
       "2           FDN15        17.50          Low Fat         0.016760   \n",
       "3           FDX07        19.20          Regular         0.000000   \n",
       "4           NCD19         8.93          Low Fat         0.000000   \n",
       "\n",
       "               Item_Type  Item_MRP Outlet_Identifier  \\\n",
       "0                  Dairy  249.8092            OUT049   \n",
       "1            Soft Drinks   48.2692            OUT018   \n",
       "2                   Meat  141.6180            OUT049   \n",
       "3  Fruits and Vegetables  182.0950            OUT010   \n",
       "4              Household   53.8614            OUT013   \n",
       "\n",
       "   Outlet_Establishment_Year Outlet_Size Outlet_Location_Type  \\\n",
       "0                       1999      Medium               Tier 1   \n",
       "1                       2009      Medium               Tier 3   \n",
       "2                       1999      Medium               Tier 1   \n",
       "3                       1998         NaN               Tier 3   \n",
       "4                       1987        High               Tier 3   \n",
       "\n",
       "         Outlet_Type  Item_Outlet_Sales  \n",
       "0  Supermarket Type1          3735.1380  \n",
       "1  Supermarket Type2           443.4228  \n",
       "2  Supermarket Type1          2097.2700  \n",
       "3      Grocery Store           732.3800  \n",
       "4  Supermarket Type1           994.7052  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8487b472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8518</th>\n",
       "      <td>FDF22</td>\n",
       "      <td>6.865</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.056783</td>\n",
       "      <td>Snack Foods</td>\n",
       "      <td>214.5218</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2778.3834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8519</th>\n",
       "      <td>FDS36</td>\n",
       "      <td>8.380</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.046982</td>\n",
       "      <td>Baking Goods</td>\n",
       "      <td>108.1570</td>\n",
       "      <td>OUT045</td>\n",
       "      <td>2002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tier 2</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>549.2850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8520</th>\n",
       "      <td>NCJ29</td>\n",
       "      <td>10.600</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.035186</td>\n",
       "      <td>Health and Hygiene</td>\n",
       "      <td>85.1224</td>\n",
       "      <td>OUT035</td>\n",
       "      <td>2004</td>\n",
       "      <td>Small</td>\n",
       "      <td>Tier 2</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>1193.1136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8521</th>\n",
       "      <td>FDN46</td>\n",
       "      <td>7.210</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.145221</td>\n",
       "      <td>Snack Foods</td>\n",
       "      <td>103.1332</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>1845.5976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8522</th>\n",
       "      <td>DRG01</td>\n",
       "      <td>14.800</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.044878</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>75.4670</td>\n",
       "      <td>OUT046</td>\n",
       "      <td>1997</td>\n",
       "      <td>Small</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>765.6700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility  \\\n",
       "8518           FDF22        6.865          Low Fat         0.056783   \n",
       "8519           FDS36        8.380          Regular         0.046982   \n",
       "8520           NCJ29       10.600          Low Fat         0.035186   \n",
       "8521           FDN46        7.210          Regular         0.145221   \n",
       "8522           DRG01       14.800          Low Fat         0.044878   \n",
       "\n",
       "               Item_Type  Item_MRP Outlet_Identifier  \\\n",
       "8518         Snack Foods  214.5218            OUT013   \n",
       "8519        Baking Goods  108.1570            OUT045   \n",
       "8520  Health and Hygiene   85.1224            OUT035   \n",
       "8521         Snack Foods  103.1332            OUT018   \n",
       "8522         Soft Drinks   75.4670            OUT046   \n",
       "\n",
       "      Outlet_Establishment_Year Outlet_Size Outlet_Location_Type  \\\n",
       "8518                       1987        High               Tier 3   \n",
       "8519                       2002         NaN               Tier 2   \n",
       "8520                       2004       Small               Tier 2   \n",
       "8521                       2009      Medium               Tier 3   \n",
       "8522                       1997       Small               Tier 1   \n",
       "\n",
       "            Outlet_Type  Item_Outlet_Sales  \n",
       "8518  Supermarket Type1          2778.3834  \n",
       "8519  Supermarket Type1           549.2850  \n",
       "8520  Supermarket Type1          1193.1136  \n",
       "8521  Supermarket Type2          1845.5976  \n",
       "8522  Supermarket Type1           765.6700  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8602fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier               0.000000\n",
       "Item_Weight                  17.165317\n",
       "Item_Fat_Content              0.000000\n",
       "Item_Visibility               0.000000\n",
       "Item_Type                     0.000000\n",
       "Item_MRP                      0.000000\n",
       "Outlet_Identifier             0.000000\n",
       "Outlet_Establishment_Year     0.000000\n",
       "Outlet_Size                  28.276428\n",
       "Outlet_Location_Type          0.000000\n",
       "Outlet_Type                   0.000000\n",
       "Item_Outlet_Sales             0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the percentage of missing values in each variable\n",
    "\n",
    "train.isnull().sum()/len(train)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb59908",
   "metadata": {},
   "source": [
    "As we can see above there are too many missing values in 2 variables. We can impute the values using appropriate methods, or we can set a threshold of, say 20% and remove the variable having more than 20% missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2243df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving missing values in  a variable\n",
    "\n",
    "a = train.isnull().sum()/len(train)*100\n",
    "\n",
    "# Saving column names in a variable\n",
    "\n",
    "variables = train.columns\n",
    "variable = []\n",
    "\n",
    "for i in range(0, 12):\n",
    "    if a[i]<=20:         # Setting the threshold as 20%\n",
    "        variable.append(variables[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27320c31",
   "metadata": {},
   "source": [
    "Now the variables to be used are stored in \"variable\", which contains only those features where the missing values are less than 20%.\n",
    "\n",
    "# 3.2 Low Variance Filter\n",
    "\n",
    "Consider a variable in our dataset where all the observations have the same value, say 1. If we use this variable, do you think it can improve the model we will build? The answer is no, because this variable will have zero variance.\n",
    "\n",
    "So, we need to calculate the variance of each variable we are given. Then drop the variables having low variance as compared to other variables in our dataset. The reason for doing this, as I mentioned above, is that variables with a low variance will not affect the target variable.\n",
    "\n",
    "Let’s first impute the missing values in the Item_Weight column using the median value of the known Item_Weight observations. For the Outlet_Size column, we will use the mode of the known Outlet_Size values to impute the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ee6a26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Item_Weight'].fillna(train['Item_Weight'].median(), inplace=True)\n",
    "train['Outlet_Size'].fillna(train['Outlet_Size'].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13cb250f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Identifier              0.0\n",
       "Item_Weight                  0.0\n",
       "Item_Fat_Content             0.0\n",
       "Item_Visibility              0.0\n",
       "Item_Type                    0.0\n",
       "Item_MRP                     0.0\n",
       "Outlet_Identifier            0.0\n",
       "Outlet_Establishment_Year    0.0\n",
       "Outlet_Size                  0.0\n",
       "Outlet_Location_Type         0.0\n",
       "Outlet_Type                  0.0\n",
       "Item_Outlet_Sales            0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check whether all the missing values been filled or not.\n",
    "\n",
    "train.isnull().sum()/len(train)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "002eee3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Item_Weight                  1.786956e+01\n",
       "Item_Visibility              2.662335e-03\n",
       "Item_MRP                     3.878184e+03\n",
       "Outlet_Establishment_Year    7.008637e+01\n",
       "Item_Outlet_Sales            2.912141e+06\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets calculate the variance of all the numerical variables.\n",
    "\n",
    "train.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b81cbf",
   "metadata": {},
   "source": [
    "Above output shows that the variance in Item_Visibility is very less compared to the other variables. We can safely drop this column. This is how we apply low variance filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3128601",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = train[['Item_Weight', 'Item_Visibility', 'Item_MRP', 'Outlet_Establishment_Year']]\n",
    "var = numeric.var()\n",
    "numeric = numeric.columns\n",
    "variable = [ ]\n",
    "for i in range(0, len(var)):\n",
    "    if var[i] >= 10:   #setting the threshold as 10%\n",
    "       variable.append(numeric[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7733e7b1",
   "metadata": {},
   "source": [
    "# 3.3 High Correlation filter\n",
    "High correlation between two variables means they have similar trends and are likely to carry similar information. This can bring down the performance of some models drastically (linear and logistic regression models, for instance). We can calculate the correlation between independent numerical variables that are numerical in nature. If the correlation coefficient crosses a certain threshold value, we can drop one of the variables (dropping a variable is highly subjective and should always be done keeping the domain in mind).\n",
    "\n",
    "As a general guideline, we should keep those variables which show a decent or high correlation with the target variable.\n",
    "\n",
    "Let’s perform the correlation calculation in Python. We will drop the dependent variable (Item_Outlet_Sales) first and save the remaining variables in a new dataframe (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ad46353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Item_Weight</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.014168</td>\n",
       "      <td>0.024951</td>\n",
       "      <td>0.007739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Item_Visibility</th>\n",
       "      <td>-0.014168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>-0.074834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Item_MRP</th>\n",
       "      <td>0.024951</td>\n",
       "      <td>-0.001315</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.005020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <td>0.007739</td>\n",
       "      <td>-0.074834</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Item_Weight  Item_Visibility  Item_MRP  \\\n",
       "Item_Weight                   1.000000        -0.014168  0.024951   \n",
       "Item_Visibility              -0.014168         1.000000 -0.001315   \n",
       "Item_MRP                      0.024951        -0.001315  1.000000   \n",
       "Outlet_Establishment_Year     0.007739        -0.074834  0.005020   \n",
       "\n",
       "                           Outlet_Establishment_Year  \n",
       "Item_Weight                                 0.007739  \n",
       "Item_Visibility                            -0.074834  \n",
       "Item_MRP                                    0.005020  \n",
       "Outlet_Establishment_Year                   1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = train.drop('Item_Outlet_Sales', 1)\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308e2082",
   "metadata": {},
   "source": [
    "Luckily we do not have any variable with high correlation in our dataset. If the correlation between a pair of variables is greater than 0.50 - 0.60, we should seriously consider dropping one of those variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14faab33",
   "metadata": {},
   "source": [
    "# 3.4 Random Forest\n",
    "Random Forest is one of the most widely used algorithms for feature selection. It comes packaged with in-built feature importance so you don’t need to program that separately. This helps us select a smaller subset of features.\n",
    "\n",
    "We need to convert the data into numeric form by applying one hot encoding, as Random Forest (Scikit-Learn Implementation) takes only numeric inputs. Let’s also drop the ID variables (Item_Identifier and Outlet_Identifier) as these are just unique numbers and hold no significant importance for us currently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4421e954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, random_state=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df = df.drop(['Item_Identifier','Outlet_Identifier'], axis=1)\n",
    "\n",
    "model = RandomForestRegressor(random_state=1, max_depth=10)\n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "model.fit(df, train.Item_Outlet_Sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3066d059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the Feature importance graph.\n",
    "\n",
    "features = df.columns\n",
    "importances = model.feature_importances_\n",
    "indices = np.argsort(importances)[-9:] # top 10 features\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='r', align='center')\n",
    "plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f585d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "feature = SelectFromModel(model)\n",
    "\n",
    "Fit = feature.fit_transform(df, train.Item_Outlet_Sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd39f7",
   "metadata": {},
   "source": [
    "# 3.5 Backward Feature Elimination\n",
    "Follow the below steps to understand and use the ‘Backward Feature Elimination’ technique:\n",
    "\n",
    "We first take all the n variables present in our dataset and train the model using them\n",
    "We then calculate the performance of the model\n",
    "Now, we compute the performance of the model after eliminating each variable (n times), i.e., we drop one variable every time and train the model on the remaining n-1 variables\n",
    "We identify the variable whose removal has produced the smallest (or no) change in the performance of the model, and then drop that variable\n",
    "Repeat this process until no variable can be dropped\n",
    "This method can be used when building Linear Regression or Logistic Regression models. Let’s look at it’s Python implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b290852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "lreg = LinearRegression()\n",
    "\n",
    "rfe = RFE(lreg, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07937e",
   "metadata": {},
   "source": [
    "# 3.6 Forward Feature Selection\n",
    "\n",
    "This is the opposite process of the Backward Elimination we saw above. Instead of eliminating features, we try to find the best features which improve the performance of the model. This technique works as follows:\n",
    "\n",
    "a. We start with a single feature. Essentially, we train the model n number of times using each feature separetly   \n",
    "b. The variable giving the best performance is selected as the starting variable   \n",
    "c. Then we repeat this process and add one variable at a time. The variable that produces the highest increase in performance is retained   \n",
    "d. We repeat this process until no significant improvement is seen in the model’s performance   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b09c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "ffs = f_regression(df, train.Item_Outlet_Sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8937a",
   "metadata": {},
   "source": [
    "This returns an array containing the F-values of the variables and the p-values corresponding to each F value. We will select the variables having F-value greater than 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "917a1022",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = []\n",
    "for i in range(0, len(df.columns)-1):\n",
    "    if ffs[0][i] >= 10:\n",
    "        variable.append(df.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df0bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54669ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
